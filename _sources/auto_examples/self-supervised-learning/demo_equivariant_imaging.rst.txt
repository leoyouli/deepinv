
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-22

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.training_utils import train, test
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 23-26

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 26-39

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 40-49

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 49-63

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      2%|▏         | 6.73M/399M [00:00<00:05, 67.3MiB/s]      4%|▎         | 14.1M/399M [00:00<00:05, 71.3MiB/s]      5%|▌         | 21.3M/399M [00:00<00:07, 50.0MiB/s]      7%|▋         | 28.9M/399M [00:00<00:06, 58.2MiB/s]      9%|▉         | 35.3M/399M [00:00<00:10, 34.8MiB/s]     11%|█         | 42.7M/399M [00:00<00:08, 43.0MiB/s]     12%|█▏        | 48.4M/399M [00:01<00:08, 40.0MiB/s]     14%|█▍        | 55.4M/399M [00:01<00:07, 46.5MiB/s]     16%|█▌        | 63.4M/399M [00:01<00:06, 54.4MiB/s]     17%|█▋        | 69.7M/399M [00:01<00:09, 34.1MiB/s]     19%|█▊        | 74.6M/399M [00:02<00:18, 17.1MiB/s]     21%|██        | 81.8M/399M [00:02<00:13, 22.9MiB/s]     22%|██▏       | 89.4M/399M [00:02<00:10, 29.8MiB/s]     24%|██▍       | 96.9M/399M [00:02<00:08, 36.9MiB/s]     26%|██▌       | 104M/399M [00:02<00:06, 44.0MiB/s]      28%|██▊       | 112M/399M [00:02<00:05, 50.4MiB/s]     30%|██▉       | 119M/399M [00:03<00:04, 56.2MiB/s]     32%|███▏      | 127M/399M [00:03<00:04, 60.8MiB/s]     34%|███▎      | 134M/399M [00:03<00:04, 64.6MiB/s]     36%|███▌      | 142M/399M [00:03<00:03, 67.8MiB/s]     38%|███▊      | 150M/399M [00:03<00:03, 70.0MiB/s]     39%|███▉      | 157M/399M [00:03<00:03, 71.7MiB/s]     41%|████▏     | 165M/399M [00:03<00:03, 72.9MiB/s]     43%|████▎     | 173M/399M [00:03<00:03, 74.1MiB/s]     45%|████▌     | 180M/399M [00:03<00:03, 60.1MiB/s]     47%|████▋     | 187M/399M [00:04<00:03, 54.1MiB/s]     48%|████▊     | 193M/399M [00:04<00:04, 42.1MiB/s]     50%|█████     | 200M/399M [00:04<00:04, 48.7MiB/s]     52%|█████▏    | 208M/399M [00:04<00:04, 41.6MiB/s]     54%|█████▎    | 214M/399M [00:04<00:04, 44.3MiB/s]     56%|█████▌    | 221M/399M [00:04<00:03, 51.4MiB/s]     57%|█████▋    | 227M/399M [00:05<00:03, 42.9MiB/s]     59%|█████▉    | 234M/399M [00:05<00:03, 49.3MiB/s]     60%|██████    | 240M/399M [00:05<00:05, 29.4MiB/s]     62%|██████▏   | 247M/399M [00:05<00:04, 36.1MiB/s]     64%|██████▍   | 254M/399M [00:05<00:03, 40.5MiB/s]     65%|██████▌   | 259M/399M [00:06<00:04, 28.8MiB/s]     67%|██████▋   | 267M/399M [00:06<00:03, 37.1MiB/s]     68%|██████▊   | 273M/399M [00:06<00:03, 35.1MiB/s]     71%|███████   | 281M/399M [00:06<00:02, 44.3MiB/s]     72%|███████▏  | 288M/399M [00:06<00:02, 42.4MiB/s]     74%|███████▍  | 296M/399M [00:06<00:02, 50.7MiB/s]     76%|███████▋  | 304M/399M [00:07<00:02, 38.5MiB/s]     78%|███████▊  | 309M/399M [00:07<00:02, 40.2MiB/s]     80%|███████▉  | 317M/399M [00:07<00:01, 49.1MiB/s]     81%|████████  | 323M/399M [00:07<00:01, 44.6MiB/s]     83%|████████▎ | 332M/399M [00:07<00:01, 52.9MiB/s]     85%|████████▍ | 338M/399M [00:07<00:01, 50.6MiB/s]     86%|████████▌ | 343M/399M [00:07<00:01, 49.4MiB/s]     88%|████████▊ | 352M/399M [00:07<00:00, 57.9MiB/s]     90%|████████▉ | 358M/399M [00:08<00:00, 52.4MiB/s]     91%|█████████▏| 364M/399M [00:08<00:00, 36.9MiB/s]     92%|█████████▏| 368M/399M [00:08<00:00, 32.4MiB/s]     95%|█████████▍| 377M/399M [00:08<00:00, 42.2MiB/s]     96%|█████████▌| 383M/399M [00:08<00:00, 44.9MiB/s]     98%|█████████▊| 389M/399M [00:09<00:00, 34.7MiB/s]     99%|█████████▊| 393M/399M [00:09<00:00, 31.6MiB/s]    100%|█████████▉| 397M/399M [00:09<00:00, 31.4MiB/s]    100%|██████████| 399M/399M [00:09<00:00, 42.6MiB/s]




.. GENERATED FROM PYTHON SOURCE LINES 64-68

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 68-98

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    /opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
      warnings.warn(
    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 116.60it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/19 [00:00<?, ?it/s]     95%|█████████▍| 18/19 [00:00<00:00, 176.54it/s]    100%|██████████| 19/19 [00:00<00:00, 181.62it/s]
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 99-104

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 104-149

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            train=True,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 150-163

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 163-188

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]     95%|█████████▍| 2.05M/2.17M [00:00<00:00, 18.1MB/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 18.9MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 189-193

Train the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 193-221

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    train(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        ckp_interval=10,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
      0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:02<?, ?it/s, eval_psnr=37.4, loss_mc=1.36e-5, loss_ei=0.000175, total_loss=0.000189, train_psnr=36.2]    Epoch 1:  20%|██        | 1/5 [00:02<00:09,  2.26s/it, eval_psnr=37.4, loss_mc=1.36e-5, loss_ei=0.000175, total_loss=0.000189, train_psnr=36.2]    Epoch 1:  20%|██        | 1/5 [00:02<00:09,  2.26s/it, eval_psnr=37.4, loss_mc=1.36e-5, loss_ei=0.000175, total_loss=0.000189, train_psnr=36.2]    Epoch 1:  20%|██        | 1/5 [00:04<00:09,  2.26s/it, eval_psnr=37.4, loss_mc=1.35e-5, loss_ei=0.000122, total_loss=0.000136, train_psnr=38]      Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.27s/it, eval_psnr=37.4, loss_mc=1.35e-5, loss_ei=0.000122, total_loss=0.000136, train_psnr=38]    Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.27s/it, eval_psnr=37.4, loss_mc=1.35e-5, loss_ei=0.000122, total_loss=0.000136, train_psnr=38]    Epoch 1:  40%|████      | 2/5 [00:06<00:06,  2.27s/it, eval_psnr=37.4, loss_mc=1.2e-5, loss_ei=0.000128, total_loss=0.00014, train_psnr=37.8]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.27s/it, eval_psnr=37.4, loss_mc=1.2e-5, loss_ei=0.000128, total_loss=0.00014, train_psnr=37.8]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.27s/it, eval_psnr=37.4, loss_mc=1.2e-5, loss_ei=0.000128, total_loss=0.00014, train_psnr=37.8]    Epoch 1:  60%|██████    | 3/5 [00:09<00:04,  2.27s/it, eval_psnr=37.4, loss_mc=1.3e-5, loss_ei=0.00014, total_loss=0.000153, train_psnr=37.4]    Epoch 1:  80%|████████  | 4/5 [00:09<00:02,  2.29s/it, eval_psnr=37.4, loss_mc=1.3e-5, loss_ei=0.00014, total_loss=0.000153, train_psnr=37.4]    Epoch 1:  80%|████████  | 4/5 [00:09<00:02,  2.29s/it, eval_psnr=37.4, loss_mc=1.3e-5, loss_ei=0.00014, total_loss=0.000153, train_psnr=37.4]    Epoch 1:  80%|████████  | 4/5 [00:11<00:02,  2.29s/it, eval_psnr=37.4, loss_mc=1.18e-5, loss_ei=0.000126, total_loss=0.000138, train_psnr=37.3]    Epoch 1: 100%|██████████| 5/5 [00:11<00:00,  2.29s/it, eval_psnr=37.4, loss_mc=1.18e-5, loss_ei=0.000126, total_loss=0.000138, train_psnr=37.3]    Epoch 1: 100%|██████████| 5/5 [00:11<00:00,  2.29s/it, eval_psnr=37.4, loss_mc=1.18e-5, loss_ei=0.000126, total_loss=0.000138, train_psnr=37.3]

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): HQSIteration(
          (f_step): fStepHQS()
          (g_step): gStepHQS()
        )
      )
      (init_params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0): PnP(
          (denoiser): DnCNN(
            (in_conv): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv_list): ModuleList(
              (0-4): 5 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (out_conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nl_list): ModuleList(
              (0-5): 6 x ReLU()
            )
          )
        )
      )
      (data_fidelity): ModuleList(
        (0): L2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 222-226

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 226-240

.. code-block:: Python


    plot_images = True
    method = "equivariant_imaging"

    test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: No learning, Recons., GT
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/73 [00:00<?, ?it/s]      1%|▏         | 1/73 [00:00<01:07,  1.06it/s]      3%|▎         | 2/73 [00:01<00:32,  2.16it/s]      4%|▍         | 3/73 [00:01<00:21,  3.32it/s]      5%|▌         | 4/73 [00:01<00:15,  4.47it/s]      7%|▋         | 5/73 [00:01<00:12,  5.52it/s]      8%|▊         | 6/73 [00:01<00:10,  6.44it/s]     10%|▉         | 7/73 [00:01<00:09,  7.20it/s]     11%|█         | 8/73 [00:01<00:08,  7.80it/s]     12%|█▏        | 9/73 [00:01<00:07,  8.25it/s]     14%|█▎        | 10/73 [00:01<00:07,  8.59it/s]     15%|█▌        | 11/73 [00:02<00:07,  8.84it/s]     16%|█▋        | 12/73 [00:02<00:06,  8.76it/s]     18%|█▊        | 13/73 [00:02<00:06,  8.92it/s]     19%|█▉        | 14/73 [00:02<00:06,  9.09it/s]     21%|██        | 15/73 [00:02<00:06,  9.21it/s]     22%|██▏       | 16/73 [00:02<00:06,  9.30it/s]     23%|██▎       | 17/73 [00:02<00:05,  9.36it/s]     25%|██▍       | 18/73 [00:02<00:05,  9.41it/s]     26%|██▌       | 19/73 [00:02<00:05,  9.44it/s]     27%|██▋       | 20/73 [00:02<00:05,  9.47it/s]     29%|██▉       | 21/73 [00:03<00:05,  9.47it/s]     30%|███       | 22/73 [00:03<00:05,  9.17it/s]     32%|███▏      | 23/73 [00:03<00:05,  9.20it/s]     33%|███▎      | 24/73 [00:03<00:05,  9.27it/s]     34%|███▍      | 25/73 [00:03<00:05,  9.33it/s]     36%|███▌      | 26/73 [00:03<00:05,  9.36it/s]     37%|███▋      | 27/73 [00:03<00:04,  9.37it/s]     38%|███▊      | 28/73 [00:03<00:04,  9.39it/s]     40%|███▉      | 29/73 [00:03<00:04,  9.40it/s]     41%|████      | 30/73 [00:04<00:04,  9.40it/s]     42%|████▏     | 31/73 [00:04<00:04,  9.41it/s]     44%|████▍     | 32/73 [00:04<00:04,  9.09it/s]     45%|████▌     | 33/73 [00:04<00:04,  9.20it/s]     47%|████▋     | 34/73 [00:04<00:04,  9.28it/s]     48%|████▊     | 35/73 [00:04<00:04,  9.34it/s]     49%|████▉     | 36/73 [00:04<00:03,  9.37it/s]     51%|█████     | 37/73 [00:04<00:03,  9.40it/s]     52%|█████▏    | 38/73 [00:04<00:03,  9.42it/s]     53%|█████▎    | 39/73 [00:05<00:03,  9.43it/s]     55%|█████▍    | 40/73 [00:05<00:03,  9.43it/s]     56%|█████▌    | 41/73 [00:05<00:03,  9.44it/s]     58%|█████▊    | 42/73 [00:05<00:03,  9.11it/s]     59%|█████▉    | 43/73 [00:05<00:03,  9.21it/s]     60%|██████    | 44/73 [00:05<00:03,  9.28it/s]     62%|██████▏   | 45/73 [00:05<00:02,  9.34it/s]     63%|██████▎   | 46/73 [00:05<00:02,  9.37it/s]     64%|██████▍   | 47/73 [00:05<00:02,  9.40it/s]     66%|██████▌   | 48/73 [00:05<00:02,  9.42it/s]     67%|██████▋   | 49/73 [00:06<00:02,  9.44it/s]     68%|██████▊   | 50/73 [00:06<00:02,  9.45it/s]     70%|██████▉   | 51/73 [00:06<00:02,  9.46it/s]     71%|███████   | 52/73 [00:06<00:02,  9.13it/s]     73%|███████▎  | 53/73 [00:06<00:02,  9.21it/s]     74%|███████▍  | 54/73 [00:06<00:02,  9.28it/s]     75%|███████▌  | 55/73 [00:06<00:01,  9.33it/s]     77%|███████▋  | 56/73 [00:06<00:01,  9.37it/s]     78%|███████▊  | 57/73 [00:06<00:01,  9.41it/s]     79%|███████▉  | 58/73 [00:07<00:01,  9.43it/s]     81%|████████  | 59/73 [00:07<00:01,  9.44it/s]     82%|████████▏ | 60/73 [00:07<00:01,  9.45it/s]     84%|████████▎ | 61/73 [00:07<00:01,  9.45it/s]     85%|████████▍ | 62/73 [00:07<00:01,  9.12it/s]     86%|████████▋ | 63/73 [00:07<00:01,  9.22it/s]     88%|████████▊ | 64/73 [00:07<00:00,  9.29it/s]     89%|████████▉ | 65/73 [00:07<00:00,  9.34it/s]     90%|█████████ | 66/73 [00:07<00:00,  9.39it/s]     92%|█████████▏| 67/73 [00:08<00:00,  9.39it/s]     93%|█████████▎| 68/73 [00:08<00:00,  9.36it/s]     95%|█████████▍| 69/73 [00:08<00:00,  9.32it/s]     96%|█████████▌| 70/73 [00:08<00:00,  9.29it/s]     97%|█████████▋| 71/73 [00:08<00:00,  9.29it/s]     99%|█████████▊| 72/73 [00:08<00:00,  8.96it/s]    100%|██████████| 73/73 [00:08<00:00,  9.05it/s]    100%|██████████| 73/73 [00:08<00:00,  8.41it/s]
    Test PSNR: No learning rec.: 28.70+-3.14 dB | Model: 36.22+-2.15 dB. 

    (36.223185918102523, 2.1478173985231379, 28.70320398513585, 3.1404058468490259)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 38.649 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
